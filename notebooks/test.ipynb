{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmark detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torchmetrics import MinMetric, MeanMetric\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from PIL import Image\n",
    "from xml.etree import ElementTree as ET\n",
    "from typing import Any, Tuple\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning import LightningDataModule, LightningModule, Trainer\n",
    "import torch\n",
    "import cv2\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, path, data_dir: str) -> None:\n",
    "        super().__init__()\n",
    "        self.images = ET.parse(path).getroot()[2]\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        data = self.images[index]\n",
    "        true_width = int(data.attrib['width'])\n",
    "        true_height = int(data.attrib['height'])\n",
    "        left = int(data[0].attrib['left'])\n",
    "        top = int(data[0].attrib['top'])\n",
    "        width = int(data[0].attrib['width'])\n",
    "        height = int(data[0].attrib['height'])\n",
    "        transforms = A.Compose([\n",
    "            A.Crop(x_min=max(0, left), y_min=max(0, top), x_max=min(left + width, true_width), \n",
    "                   y_max=min(top + height, true_height)),\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "        landmarks = []\n",
    "        for landmark in data[0]:\n",
    "            landmarks.append((float(landmark.attrib['x']), float(landmark.attrib['y'])))\n",
    "        transformed = transforms(\n",
    "            image=np.array(Image.open(self.data_dir + data.attrib['file']).convert('RGB')), \n",
    "            keypoints=landmarks)\n",
    "        image = transformed['image']\n",
    "        landmarks = torch.tensor(transformed['keypoints']).flatten() / 224\n",
    "        return image, landmarks\n",
    "    \n",
    "class FaceDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_val_split: Tuple[int, int] = (0.8, 0.2),\n",
    "        data_dir: str = \"./data/\",\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_workers = num_workers\n",
    "        self.train_path = data_dir + 'ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml'\n",
    "        self.test_path = data_dir + 'ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test.xml'\n",
    "        train_dataset = FaceDataset(self.train_path, data_dir=data_dir + 'ibug_300W_large_face_landmark_dataset/')\n",
    "        (self.data_train, self.data_val) = random_split(\n",
    "            dataset=train_dataset,\n",
    "            lengths=train_val_split,\n",
    "            generator=torch.Generator().manual_seed(42),\n",
    "        )\n",
    "        self.data_test: Dataset = FaceDataset(self.test_path, data_dir=data_dir + 'ibug_300W_large_face_landmark_dataset/')\n",
    "\n",
    "        self.batch_size_per_device = batch_size\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        return 136\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "class FaceModule(LightningModule):\n",
    "    def __init__(self, n_classes: int = 136, lr: float = 1e-4) -> None:\n",
    "        super().__init__()\n",
    "        self.i = 0\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.net = mobilenet_v2(pretrained=True)\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.net.classifier[1] = torch.nn.Linear(self.net.classifier[1].in_features, self.hparams.n_classes)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "         # for averaging loss across batches\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "\n",
    "        # for tracking best so far validation accuracy\n",
    "        self.val_loss_best = MinMetric()\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        self.train_loss.reset()\n",
    "        self.val_loss.reset()\n",
    "        self.val_loss_best.reset()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "    \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.train_loss(loss)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.val_loss(loss)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.test_loss(loss)\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, img: Image, left, top, width, height, true_width, true_height) -> Any:\n",
    "        t = A.Compose([\n",
    "            A.Crop(x_min=max(0, left), y_min=max(0, top), x_max=min(left + width, true_width), \n",
    "                   y_max=min(top + height, true_height)),\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        transformed = t(image=np.array(img))\n",
    "        image = transformed['image']\n",
    "        image = image.to(self.device)\n",
    "        logits = self.forward(image.unsqueeze(0))\n",
    "        return logits.squeeze(0)\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        def draw(image, landmarks, top, left):\n",
    "            # draw image and point landmarks\n",
    "            image = image.copy()\n",
    "            for landmark in landmarks:\n",
    "                cv2.circle(image, (int(landmark[0]) + left, int(landmark[1]) + top), 1, (0, 255, 0), 5)\n",
    "            return image\n",
    "            \n",
    "        acc = self.val_loss.compute()  # get current val loss\n",
    "        self.val_loss_best(acc)  # update best so far val loss\n",
    "        self.log(\"val_loss_best\", self.val_loss_best.compute(), sync_dist=True, prog_bar=True)\n",
    "\n",
    "        img = Image.open('..\\data\\ibug_300W_large_face_landmark_dataset\\lfpw\\trainset\\image_0457.png')\n",
    "        landmarks = self.predict_step(img, left=74, top=78, width=138, height=140, true_width=350, true_height=464)\n",
    "        landmarks = landmarks.cpu().detach().numpy().reshape((68, 2)) * np.array([138, 140])\n",
    "        landmarks = landmarks.astype(int)\n",
    "        img = draw(np.array(img), landmarks, top=78, left=74)\n",
    "        img = Image.fromarray(img).save(f'../results/{self.i}.jpg')\n",
    "        self.i += 1\n",
    "        # self.log('example', wandb.Image(draw(np.array(img), landmarks)))\n",
    "\n",
    "    def save_to_state_dict(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_from_state_dict(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_path = './lightning_logs/version_35/checkpoints/epoch=9-step=840.ckpt'\n",
    "model = FaceModule.load_from_checkpoint(model_path)\n",
    "# wandb.login()\n",
    "# logger = WandbLogger(project=\"face_landmark\", log_model=True)\n",
    "datamodule = FaceDataModule(data_dir='../data/', batch_size=64, num_workers=0, train_val_split=(0.8, 0.2))\n",
    "# model = FaceModule(n_classes=datamodule.num_classes, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, landmarks = next(iter(datamodule.train_dataloader()))\n",
    "\n",
    "image_1 = images[3]\n",
    "landmarks_1 = landmarks[3]\n",
    "\n",
    "image_1 = image_1.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "landmarks_1 = landmarks_1.reshape((68, 2)) * np.array([224, 224])\n",
    "# landmarks_1 = landmarks_1.tolist()\n",
    "\n",
    "image_1 = image_1.copy()\n",
    "for landmark in landmarks_1:\n",
    "    cv2.circle(image_1, (int(landmark[0]), int(landmark[1])), 1, (0, 255, 0), 5)\n",
    "\n",
    "# cv2.imwrite('../results/0.jpg', image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type        | Params\n",
      "----------------------------------------------\n",
      "0 | net           | MobileNetV2 | 2.4 M \n",
      "1 | criterion     | MSELoss     | 0     \n",
      "2 | train_loss    | MeanMetric  | 0     \n",
      "3 | val_loss      | MeanMetric  | 0     \n",
      "4 | test_loss     | MeanMetric  | 0     \n",
      "5 | val_loss_best | MinMetric   | 0     \n",
      "----------------------------------------------\n",
      "174 K     Trainable params\n",
      "2.2 M     Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.592     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb03dc8e8a074189b40c5e6f7e3ca98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '..\\\\data\\\\ibug_300W_large_face_landmark_dataset\\\\lfpw\\trainset\\\\image_0457.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\Filter\\face-landmark-mobilenet\\notebooks\\test.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m)],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# logger=logger,\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mmodel, datamodule\u001b[39m=\u001b[39;49mdatamodule)\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[0;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    546\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    547\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    575\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    576\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    577\u001b[0m     ckpt_path,\n\u001b[0;32m    578\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    580\u001b[0m )\n\u001b[1;32m--> 581\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    583\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1034\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m   1033\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1034\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[0;32m   1035\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1036\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1063\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1060\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1063\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1065\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1067\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[0;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:141\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store_dataloader_outputs()\n\u001b[1;32m--> 141\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_end()\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:253\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39m_evaluation_epoch_end()\n\u001b[0;32m    252\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_evaluation_epoch_end()\n\u001b[0;32m    255\u001b[0m logged_outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logged_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logged_outputs, []  \u001b[39m# free memory\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:329\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    327\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_epoch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_epoch_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    328\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, hook_name)\n\u001b[1;32m--> 329\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(trainer, hook_name)\n\u001b[0;32m    331\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    160\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "\u001b[1;32md:\\Filter\\face-landmark-mobilenet\\notebooks\\test.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loss_best(acc)  \u001b[39m# update best so far val loss\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mval_loss_best\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loss_best\u001b[39m.\u001b[39mcompute(), sync_dist\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(\u001b[39m'\u001b[39;49m\u001b[39m..\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mibug_300W_large_face_landmark_dataset\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mlfpw\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39mrainset\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mimage_0457.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m landmarks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_step(img, left\u001b[39m=\u001b[39m\u001b[39m74\u001b[39m, top\u001b[39m=\u001b[39m\u001b[39m78\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m138\u001b[39m, height\u001b[39m=\u001b[39m\u001b[39m140\u001b[39m, true_width\u001b[39m=\u001b[39m\u001b[39m350\u001b[39m, true_height\u001b[39m=\u001b[39m\u001b[39m464\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Filter/face-landmark-mobilenet/notebooks/test.ipynb#W5sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m landmarks \u001b[39m=\u001b[39m landmarks\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape((\u001b[39m68\u001b[39m, \u001b[39m2\u001b[39m)) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m138\u001b[39m, \u001b[39m140\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\letua\\miniconda3\\envs\\ml\\lib\\site-packages\\PIL\\Image.py:3131\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3128\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3130\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3131\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3132\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3134\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '..\\\\data\\\\ibug_300W_large_face_landmark_dataset\\\\lfpw\\trainset\\\\image_0457.png'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\")],\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    # logger=logger,\n",
    ")\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_to_torchscript('./model.ptl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchmetrics import MinMetric, MeanMetric\n",
    "from PIL import Image\n",
    "from xml.etree import ElementTree as ET\n",
    "from typing import Any, Tuple\n",
    "from lightning import LightningDataModule, LightningModule, Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import mobilenet_v2\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import albumentations as A\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class FaceBoxDataset(Dataset):\n",
    "    def __init__(self, path, data_dir: str) -> None:\n",
    "        super().__init__()\n",
    "        self.images = ET.parse(path).getroot()[2]\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def build_label(self, keypoints):\n",
    "        x1, y1 = keypoints[0]\n",
    "        x2, y2 = keypoints[1]\n",
    "        x1 = max(0, x1)\n",
    "        y1 = max(0, y1)\n",
    "        x2 = min(243, x2)\n",
    "        y2 = min(243, y2)\n",
    "        # image array shape (244, 244) with only x1, y1, x2, y2 is 1 and others are 0\n",
    "        label = np.zeros((244, 244))\n",
    "        label[int(y1)][int(x1)] = label[int(y2)][int(x2)] = 1\n",
    "        return torch.tensor(label, dtype=torch.float32).flatten()\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        data = self.images[index]\n",
    "        left = int(data[0].attrib['left'])\n",
    "        top = int(data[0].attrib['top'])\n",
    "        width = int(data[0].attrib['width'])\n",
    "        height = int(data[0].attrib['height'])\n",
    "        box_keypoints = [\n",
    "            (left, top),\n",
    "            (left + width, top + height),\n",
    "        ]\n",
    "        transforms = A.Compose([\n",
    "            A.Resize(244, 244),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "        transformed = transforms(\n",
    "            image=np.array(Image.open(self.data_dir + data.attrib['file']).convert('RGB')), \n",
    "            keypoints=box_keypoints)\n",
    "        image = transformed['image']\n",
    "        box_keypoints = transformed['keypoints']\n",
    "        label = self.build_label(box_keypoints)\n",
    "        return image, label\n",
    "    \n",
    "class FaceBoxDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_val_split: Tuple[int, int] = (0.8, 0.2),\n",
    "        data_dir: str = \"./data/\",\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_workers = num_workers\n",
    "        self.train_path = data_dir + 'ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml'\n",
    "        self.test_path = data_dir + 'ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test.xml'\n",
    "        train_dataset = FaceBoxDataset(self.train_path, data_dir=data_dir + 'ibug_300W_large_face_landmark_dataset/')\n",
    "        (self.data_train, self.data_val) = random_split(\n",
    "            dataset=train_dataset,\n",
    "            lengths=train_val_split,\n",
    "            generator=torch.Generator().manual_seed(42),\n",
    "        )\n",
    "        self.data_test: Dataset = FaceBoxDataset(self.test_path, data_dir=data_dir + 'ibug_300W_large_face_landmark_dataset/')\n",
    "\n",
    "        self.batch_size_per_device = batch_size\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        return 4\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader[Any]:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader[Any]:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader[Any]:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "class FaceBoxModule(LightningModule):\n",
    "    def __init__(self, n_classes: int = 4, lr: float = 1e-4) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        # self.net = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v2', pretrained=True)\n",
    "        self.net = mobilenet_v2(pretrained=False)\n",
    "        self.net.classifier = torch.nn.Linear(1280, self.hparams.n_classes)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "         # for averaging loss across batches\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "\n",
    "        # for tracking best so far validation accuracy\n",
    "        self.val_loss_best = MinMetric()\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        self.train_loss.reset()\n",
    "        self.val_loss.reset()\n",
    "        self.val_loss_best.reset()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "    \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.train_loss(loss)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.val_loss(loss)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.test_loss(loss)\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, img: Image) -> Any:\n",
    "        t = A.Compose([\n",
    "            A.Resize(244, 244),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        transformed = t(image=np.array(img))\n",
    "        image = transformed['image'].to(self.device)\n",
    "        logits = self.forward(image.unsqueeze(0))\n",
    "        return logits.squeeze(0)\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        acc = self.val_loss.compute()  # get current val loss\n",
    "        self.val_loss_best(acc)  # update best so far val loss\n",
    "        self.log(\"val_loss_best\", self.val_loss_best.compute(), sync_dist=True, prog_bar=True)\n",
    "\n",
    "    def save_to_state_dict(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def save_to_torchscript(self, path: str):\n",
    "        self.net.eval()\n",
    "        scripted_model = torch.jit.script(self.net)\n",
    "        optimized_scripted_model = optimize_for_mobile(scripted_model)\n",
    "        optimized_scripted_model._save_for_lite_interpreter(path)\n",
    "\n",
    "    def load_from_state_dict(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = FaceBoxDataModule(data_dir='../data/', batch_size=16, num_workers=0, train_val_split=(0.8, 0.2))\n",
    "model = FaceBoxModule(n_classes=244**2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\")],\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retinaface import RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = RetinaFace.detect_faces('../friends.jpg')\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../0000.parquet', engine='pyarrow')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
